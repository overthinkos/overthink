depends:
  - nvidia-cuda
  - supervisord

env:
  OLLAMA_HOST: "0.0.0.0"
  OLLAMA_MODELS: "~/.ollama/models"

ports:
  - 11434

service: |
  [program:ollama]
  command=ollama serve
  autostart=true
  autorestart=true
  stdout_logfile=/dev/stdout
  stdout_logfile_maxbytes=0
  stderr_logfile=/dev/stderr
  stderr_logfile_maxbytes=0
