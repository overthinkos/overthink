depends:
  - cuda
  - supervisord

env:
  OLLAMA_HOST: "0.0.0.0"
  OLLAMA_MODELS: "~/.ollama/models"

ports:
  - 11434

volumes:
  - name: models
    path: "~/.ollama/models"

aliases:
  - name: ollama
    command: ollama

service: |
  [program:ollama]
  command=ollama serve
  autostart=true
  autorestart=true
  stdout_logfile=/dev/stdout
  stdout_logfile_maxbytes=0
  stderr_logfile=/dev/stderr
  stderr_logfile_maxbytes=0
